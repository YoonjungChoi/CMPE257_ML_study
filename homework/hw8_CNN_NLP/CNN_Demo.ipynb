{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s26_9BlcaDz"
   },
   "source": [
    "**Load the Data**\n",
    "\n",
    "The Fashion-MNIST dataset is a dataset of Zalando's article images, with 28x28 grayscale images of 70,000 fashion products from 10 categories, and 7,000 images per category. The training set has 60,000 images, and the test set has 10,000 images.\n",
    "\n",
    "The train and test images along with the labels are loaded and stored in variables train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSU-BVrkVDfh",
    "outputId": "56b169dc-0865-4c83-9e85-8067f2a9415d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(train_X,train_Y), (test_X,test_Y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGIxqotpdVaE"
   },
   "source": [
    "**Analyze the Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxyyJN5CVIjo",
    "outputId": "136d8100-95dd-4f88-dd8e-114136f8c0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28) (60000,)\n",
      "Testing data shape :  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O-4EWwBeCmO"
   },
   "source": [
    "From the above output, you can see that the training data has a shape of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x 28 dimension. Similarly, the test data has a shape of 10000 x 28 x 28 since there are 10,000 testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MJbbI2FVPsE",
    "outputId": "37266890-c0cf-47e8-d5c8-af840ae27086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  10\n",
      "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_Y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C5Zgpl4eON-"
   },
   "source": [
    "There's also a total of ten output classes that range from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "pM6Iy6tIVT0M",
    "outputId": "1929061c-ef6f-4aa3-f0a9-f79363c2ac70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth : 9')"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_Y[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKFWQtjVeVZe"
   },
   "source": [
    "The output of above two plots looks like an ankle boot, and this class is assigned a class label of 9. Similarly, other fashion products will have different labels, but similar products will have same labels. This means that all the 7,000 ankle boot images will have a class label of 9.\n",
    "\n",
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vyxVqWgoViJ-",
    "outputId": "30b8536c-47b3-4641-aa4a-fea7149492cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X.reshape(-1, 28,28, 1)\n",
    "test_X = test_X.reshape(-1, 28,28, 1)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CytYTUKXe2xH"
   },
   "source": [
    "The data right now is in an int8 format, so before you feed it into the network you need to convert its type to float32, and you also have to rescale the pixel values in range 0 - 1 inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EQXN9U9eVlhm"
   },
   "outputs": [],
   "source": [
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENwVqpCke_05"
   },
   "source": [
    "Now you need to convert the class labels into a one-hot encoding vector.\n",
    "\n",
    "In one-hot encoding, you convert the categorical data into a vector of numbers. The reason why you convert the categorical data in one hot encoding is that machine learning algorithms cannot work with categorical data directly. You generate one boolean column for each category or class. Only one of these columns could take on the value 1 for each sample. Hence, the term one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eby-4Z-_Vud7",
    "outputId": "8d4d8a5b-9bfb-4884-f104-a8bed0eb3b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 9\n",
      "After conversion to one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haE4UtwxgEIz"
   },
   "source": [
    "**Create and check shape of training and validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uSzgnUqHV1xu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDx-g4bgV6R2",
    "outputId": "d7b25dc5-459d-4be3-9060-44683c5d5881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 28, 28, 1), (12000, 28, 28, 1), (48000, 10), (12000, 10))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkyGMT3ggkjV"
   },
   "source": [
    "**The Network**\n",
    "\n",
    "The images are of size 28 x 28. You convert the image matrix to an array, rescale it between 0 and 1, reshape it so that it's of size 28 x 28 x 1, and feed this as an input to the network.\n",
    "\n",
    "Three convolutional layers:\n",
    "\n",
    "The first layer will have 32-3 x 3 filters,\n",
    "The second layer will have 64-3 x 3 filters and\n",
    "The third layer will have 128-3 x 3 filters.\n",
    "In addition, there are three max-pooling layers each of size 2 x 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BWpVoUwrWCwO"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks5RD6Czg2Jy"
   },
   "source": [
    "Set parameters.\n",
    "\n",
    "We will use a batch size of 64 using a higher batch size of 128 or 256 is also preferable it all depends on the memory. It contributes massively to determining the learning parameters and affects the prediction accuracy. You will train the network for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9RCGrqneWGYN"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5PIIftug_Zn"
   },
   "source": [
    "**Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f-3AsRibWJw9"
   },
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqIKAdGDnUX7"
   },
   "source": [
    "First add a first convolutional layer with Conv2D(). Because we're working with images! \n",
    "\n",
    "Next, add the Leaky ReLU activation function which helps the network learn non-linear decision boundaries. Since we have ten different classes, you'll need a non-linear decision boundary that could separate these ten classes which are not linearly separable.\n",
    "\n",
    "The ReLU activation function is used instead of logistic sigmoid function.\n",
    "\n",
    "Next, we'll add the max-pooling layer with MaxPooling2D() and so on.In this layer we shrink the image stack into a smaller size. Pooling is done after passing through the activation layer.\n",
    "\n",
    "The last layer is a Dense layer that has a softmax activation function with 10 units, which is needed for this multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb1dj4sbWYIj"
   },
   "source": [
    "**Compile the Model**\n",
    "\n",
    "After the model is created, we compile it using the Adam optimizer, one of the most popular optimization algorithms. Additionally, you specify the loss type which is categorical cross entropy which is used for multi-class classification, you can also use binary cross-entropy as the loss function. Lastly, we specify the metrics as accuracy which you want to analyze while the model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "10PHoNLHWTfT"
   },
   "outputs": [],
   "source": [
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8399ozeWaDL",
    "outputId": "1b33fb0d-fb77-45c1-a2ca-120c3060f9a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 356,234\n",
      "Trainable params: 356,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMHlhh7gWm1O"
   },
   "source": [
    "**Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5fOuF-yWh2Q",
    "outputId": "82cf88d1-6e78-4072-91e7-06cbda927a4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.6844 - accuracy: 0.7485 - val_loss: 0.3623 - val_accuracy: 0.8653\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.3016 - accuracy: 0.8916 - val_loss: 0.2801 - val_accuracy: 0.8967\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.2418 - accuracy: 0.9122 - val_loss: 0.2451 - val_accuracy: 0.9103\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.2053 - accuracy: 0.9252 - val_loss: 0.2356 - val_accuracy: 0.9136\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.1783 - accuracy: 0.9329 - val_loss: 0.2234 - val_accuracy: 0.9189\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.1521 - accuracy: 0.9433 - val_loss: 0.2419 - val_accuracy: 0.9153\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.1353 - accuracy: 0.9481 - val_loss: 0.2401 - val_accuracy: 0.9170\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.1121 - accuracy: 0.9590 - val_loss: 0.2353 - val_accuracy: 0.9211\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0927 - accuracy: 0.9643 - val_loss: 0.2365 - val_accuracy: 0.9237\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 80s 107ms/step - loss: 0.0812 - accuracy: 0.9696 - val_loss: 0.2529 - val_accuracy: 0.9237\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0684 - accuracy: 0.9742 - val_loss: 0.2646 - val_accuracy: 0.9264\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 80s 107ms/step - loss: 0.0551 - accuracy: 0.9797 - val_loss: 0.2837 - val_accuracy: 0.9243\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 81s 107ms/step - loss: 0.0513 - accuracy: 0.9815 - val_loss: 0.3493 - val_accuracy: 0.9189\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0420 - accuracy: 0.9851 - val_loss: 0.3354 - val_accuracy: 0.9195\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0389 - accuracy: 0.9846 - val_loss: 0.3662 - val_accuracy: 0.9188\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0333 - accuracy: 0.9881 - val_loss: 0.3943 - val_accuracy: 0.9233\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0326 - accuracy: 0.9877 - val_loss: 0.4161 - val_accuracy: 0.9193\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 0.4277 - val_accuracy: 0.9211\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 81s 108ms/step - loss: 0.0280 - accuracy: 0.9897 - val_loss: 0.4552 - val_accuracy: 0.9210\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 80s 107ms/step - loss: 0.0292 - accuracy: 0.9889 - val_loss: 0.4668 - val_accuracy: 0.9192\n"
     ]
    }
   ],
   "source": [
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BluMEkjlW2Cy"
   },
   "source": [
    "**Model Evaluation on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-sk3ORfWqj5",
    "outputId": "4a0369ca-61a7-4b93-d5f1-8d1e8f1f4042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9143999814987183\n"
     ]
    }
   ],
   "source": [
    "test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCmxgIa3XP3v"
   },
   "source": [
    "**Reference:**\n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/style_transfer\n",
    "\n",
    "https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f\n",
    "\n",
    "https://www.edureka.co/blog/convolutional-neural-network/\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN_Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
