{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) What are the main motivations for reducing a dataset’s dimensionality? - 0.5 points\n",
    "### What are the main drawbacks? - 0.5 points"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main motivations : \n",
    "    To speed up a subsequent training algorithm (in some cases, it may even remove noise and redundant features, making the training algorithm perform better)\n",
    "    To visualize the data gain insights on the most important features. Simply to save space(compression)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " main drawbacks : \n",
    "    It may lead to some amount of data loss.\n",
    "    PCA tends to find linear correlations between variables, which is sometimes undesirable.\n",
    "    PCA fails in cases where mean and covariance are not enough to define datasets.\n",
    "    We may not know how many principal components to keep-in practice, some thumb rules are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset (given below) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Split it into a training set and a test set\n",
    "### Take the first 60,000 instances for training, and the remaining 10,000 for testing. - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = StandardScaler().fit_transform(mnist.data)\n",
    "X_train = data[:-10000,:]\n",
    "X_test = data[-10000:,:]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_train = mnist.target[:-10000]\n",
    "y_test = mnist.target[-10000:]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train a Random Forest classifier on the dataset and time how long it takes, - 1 point\n",
    "### then evaluate the resulting model on the test set. - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=123456)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=123456)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model takes 01:29.70\n",
    "After StandardScaler, it takes02:55.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(f'accuracy score: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. - 2 + 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 332)\n",
      "[0.05642719 0.04041226 0.03738273 0.02893    0.02520752 0.02192549\n",
      " 0.01914282 0.01740684 0.01532232 0.01396087 0.01342175 0.01201421\n",
      " 0.01113962 0.01090582 0.01027986 0.00994955 0.00931255 0.00919635\n",
      " 0.008886   0.00863195 0.00821741 0.00798417 0.00762573 0.00742315\n",
      " 0.0071657  0.00689314 0.00681399 0.00654588 0.00627293 0.00610345\n",
      " 0.00597261 0.00589304 0.00567358 0.00559358 0.00552473 0.00534443\n",
      " 0.00527593 0.00515841 0.00505498 0.00477438 0.00476312 0.00465155\n",
      " 0.00453454 0.00445757 0.00442313 0.00437877 0.00437294 0.00427724\n",
      " 0.00424808 0.00418524 0.00404059 0.00396258 0.00393176 0.00390562\n",
      " 0.00386444 0.00377501 0.00373883 0.00368328 0.00360377 0.0035637\n",
      " 0.00349289 0.00344527 0.00343239 0.00341    0.00334463 0.00332107\n",
      " 0.00329803 0.00319433 0.0031711  0.00315431 0.00309941 0.00305782\n",
      " 0.00305095 0.0030396  0.00296635 0.00292958 0.00291295 0.00290232\n",
      " 0.00288608 0.00287022 0.00284514 0.00281524 0.00279245 0.00278629\n",
      " 0.00278233 0.00276733 0.0027542  0.00272901 0.0026874  0.00268139\n",
      " 0.00267354 0.00263003 0.00262155 0.00258872 0.00258429 0.002571\n",
      " 0.002519   0.00250904 0.00248094 0.00244092 0.00243607 0.00241671\n",
      " 0.00238999 0.00236394 0.00235632 0.00232326 0.00229151 0.00225292\n",
      " 0.00223891 0.0022322  0.00219714 0.00219026 0.00215303 0.00213447\n",
      " 0.00210763 0.00210123 0.00206525 0.00205073 0.00202906 0.00198907\n",
      " 0.00197597 0.00196588 0.00195394 0.00194545 0.00192834 0.00191505\n",
      " 0.0018926  0.001884   0.00186304 0.0018189  0.00180676 0.00179009\n",
      " 0.00178407 0.00176999 0.00175852 0.0017478  0.00173304 0.001728\n",
      " 0.00169433 0.00167648 0.00165855 0.00164975 0.00164603 0.00164149\n",
      " 0.00161138 0.00160413 0.00158927 0.00156601 0.00156231 0.00154815\n",
      " 0.00153566 0.00152617 0.00151562 0.00149189 0.0014888  0.00147098\n",
      " 0.00146274 0.00144916 0.00144202 0.00142795 0.00142245 0.00142043\n",
      " 0.00140016 0.00139306 0.00139149 0.00139049 0.00138895 0.00138616\n",
      " 0.00138451 0.00136449 0.00135787 0.00135151 0.001349   0.00133797\n",
      " 0.00132635 0.00131349 0.00129857 0.00129275 0.00128703 0.00127087\n",
      " 0.00126384 0.00125841 0.00123504 0.00122147 0.00121683 0.00121135\n",
      " 0.00119987 0.0011959  0.00117699 0.00117216 0.00116486 0.00115194\n",
      " 0.00114869 0.00114259 0.00113339 0.00112363 0.0011098  0.0010857\n",
      " 0.00108128 0.00106264 0.00105589 0.00104707 0.00104297 0.00102776\n",
      " 0.00101473 0.00100924 0.00099916 0.00099336 0.00098883 0.00097517\n",
      " 0.00096859 0.00096289 0.00095418 0.00094574 0.00094345 0.00093592\n",
      " 0.00092743 0.00090978 0.00090247 0.00089122 0.0008811  0.00087677\n",
      " 0.00087115 0.00086639 0.0008581  0.00085287 0.00084026 0.0008293\n",
      " 0.00082325 0.00081886 0.00081504 0.00080889 0.00080534 0.00079901\n",
      " 0.00078771 0.00078122 0.00077733 0.00077151 0.00075642 0.00075139\n",
      " 0.00074756 0.00073904 0.00073446 0.00072745 0.00072132 0.00071168\n",
      " 0.00070254 0.00070072 0.0006958  0.00068794 0.00068262 0.00067936\n",
      " 0.00067621 0.00066319 0.00066042 0.00065928 0.00065277 0.00063943\n",
      " 0.00063772 0.00063592 0.0006307  0.00062632 0.00061863 0.00061657\n",
      " 0.00060856 0.00060434 0.00059843 0.00058724 0.00058085 0.000576\n",
      " 0.00057315 0.00057071 0.00056459 0.00056026 0.00055714 0.00054952\n",
      " 0.0005471  0.00054404 0.00054158 0.00053849 0.00053598 0.00052652\n",
      " 0.00052487 0.00051799 0.00051499 0.00051252 0.00050578 0.00050238\n",
      " 0.00049864 0.00049624 0.0004944  0.00049112 0.00048178 0.00047847\n",
      " 0.00047651 0.00047552 0.00047147 0.00046091 0.00045825 0.00044977\n",
      " 0.00044841 0.00044791 0.00044299 0.0004388  0.00043726 0.00043261\n",
      " 0.00043036 0.0004228  0.00041957 0.00041683 0.00041542 0.00041235\n",
      " 0.00041092 0.00040894 0.00040756 0.00040225 0.00039977 0.00039671\n",
      " 0.00039165 0.00038918 0.00038757 0.0003866  0.00038275 0.00037874\n",
      " 0.00037818 0.00037467 0.00037178 0.00036996 0.00036669 0.00036099\n",
      " 0.00035912 0.00035849]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd \n",
    "\n",
    "data = StandardScaler().fit_transform(mnist.data)\n",
    "\n",
    "pca=PCA(n_components=0.95)\n",
    "pca_data=pca.fit_transform(data)\n",
    "df_pca_data=pd.DataFrame(pca_data)\n",
    "\n",
    "print(df_pca_data.shape)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9500311796713796\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Together, the first ~ 154 principal components contain 95% (0.9500311796713796) of the information. The first principal component contains 5.6% of the variance and the second principal component contains 4% of the variance.  The 332~784 component contains the rest of the variance of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 332)\n",
      "(10000, 332)\n"
     ]
    }
   ],
   "source": [
    "X_train_pca = df_pca_data.iloc[:-10000,:]\n",
    "X_test_pca = df_pca_data.iloc[-10000:,:]\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)  Train a new Random Forest classifier on the reduced dataset and see how long it takes. - 1 point\n",
    "### Was training much faster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=123456)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=123456)\n",
    "rf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model takes 03:01.81. after StandardScaler, it takes 03:47.30\n",
    "When I used PCA, the model is much slower than above model without PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Next evaluate the classifier on the test set: how does it compare to the previous classifier? - 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted = rf.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(f'accuracy score: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous model without using PCA, I got accuracy 0.97 with 784 features, and took 02:55. From second model with using PCA, I got 0.938 with 332 features, and took 03:47. \n",
    "Apparently, The feature reduction via PCA is useful to speed up to compute and have good accuracy. However, in the above case, using PCA could cause much slower and less accurcay even though there is significant reduction of data dimentionality. I guess reason why PCA is not helping to improve model is that features either have non-linear relationships or no relationships at all. Also, the performance could vary depending on the random forest.  I think we have to take into consideration using PCA according to \"If the variables are correlated, PCA can achieve dimension reduction. If not, PCA just orders them according to their variances\". PCA is not always good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
